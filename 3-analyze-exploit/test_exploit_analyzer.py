#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test script for the exploit analyzer tool
"""

import os
import sys
import unittest
import tempfile
from unittest.mock import patch, MagicMock
from pathlib import Path

# Add the current directory to the path to import the analyzer
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

try:
    from exploit_analyzer import ExploitAnalyzer, ExploitAnalysis
except ImportError:
    # Fallback import for the main module
    import importlib.util
    spec = importlib.util.spec_from_file_location("exploit_analyzer", "3-analyze-exploit.py")
    exploit_analyzer = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(exploit_analyzer)
    ExploitAnalyzer = exploit_analyzer.ExploitAnalyzer
    ExploitAnalysis = exploit_analyzer.ExploitAnalysis


class TestExploitAnalyzer(unittest.TestCase):
    """Test cases for the ExploitAnalyzer"""
    
    def setUp(self):
        """Set up test fixtures"""
        # Create a mock LLM response
        self.mock_analysis_result = {
            "vulnerability_type": "Path Traversal",
            "cve_id": "CVE-2023-32235",
            "severity": "High",
            "target_software": "Ghost CMS < 5.42.1",
            "attack_vector": "Network",
            "impact_description": "Allows reading arbitrary files within theme folder",
            "technical_details": {
                "method": "Directory traversal via /assets/built/ endpoint",
                "encoding_bypasses": ["URL encoding", "Double encoding"]
            },
            "payloads_identified": [
                "../../package.json",
                "../../../.env",
                "..%2f..%2fconfig.json"
            ],
            "indicators_of_compromise": [
                "HTTP requests to /assets/built/ with ../ sequences",
                "Successful 200 responses to traversal attempts",
                "Access to sensitive files like .env, package.json"
            ],
            "mitigation_recommendations": [
                "Update Ghost CMS to version 5.42.1 or later",
                "Implement proper input validation",
                "Use allow-list for accessible files",
                "Monitor for directory traversal patterns"
            ],
            "confidence_score": 0.95
        }
    
    @patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key'})
    def test_analyzer_initialization(self):
        """Test that the analyzer initializes correctly"""
        with patch('exploit_analyzer.ChatOpenAI') as mock_llm:
            analyzer = ExploitAnalyzer()
            self.assertIsNotNone(analyzer)
            self.assertIsNotNone(analyzer.llm)
            self.assertIsNotNone(analyzer.parser)
            self.assertIsNotNone(analyzer.chain)
    
    @patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key'})
    def test_analyze_file_success(self):
        """Test successful file analysis"""
        # Create a temporary exploit file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write("""
#!/usr/bin/env python3
# CVE-2023-32235 Path Traversal Exploit
import requests

def exploit(url):
    payload = "../../package.json"
    response = requests.get(f"{url}/assets/built/{payload}")
    return response.text
""")
            temp_file = f.name
        
        try:
            # Mock the LangChain chain
            with patch('exploit_analyzer.ChatOpenAI'), \
                 patch.object(ExploitAnalyzer, '_setup_analysis_chain'), \
                 patch.object(ExploitAnalyzer, 'chain') as mock_chain:
                
                mock_chain.invoke.return_value = self.mock_analysis_result
                
                analyzer = ExploitAnalyzer()
                result = analyzer.analyze_file(temp_file)
                
                self.assertIsInstance(result, ExploitAnalysis)
                self.assertEqual(result.vulnerability_type, "Path Traversal")
                self.assertEqual(result.cve_id, "CVE-2023-32235")
                self.assertEqual(result.severity, "High")
                self.assertEqual(result.confidence_score, 0.95)
                self.assertTrue(len(result.payloads_identified) > 0)
                
        finally:
            # Clean up
            os.unlink(temp_file)
    
    def test_analyze_nonexistent_file(self):
        """Test analysis of non-existent file"""
        with patch('exploit_analyzer.ChatOpenAI'), \
             patch.object(ExploitAnalyzer, '_setup_analysis_chain'):
            
            analyzer = ExploitAnalyzer()
            
            with self.assertRaises(FileNotFoundError):
                analyzer.analyze_file("/path/that/does/not/exist.py")
    
    def test_analyze_empty_file(self):
        """Test analysis of empty file"""
        # Create an empty temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            temp_file = f.name
        
        try:
            with patch('exploit_analyzer.ChatOpenAI'), \
                 patch.object(ExploitAnalyzer, '_setup_analysis_chain'):
                
                analyzer = ExploitAnalyzer()
                
                with self.assertRaises(ValueError):
                    analyzer.analyze_file(temp_file)
                    
        finally:
            os.unlink(temp_file)
    
    @patch.dict(os.environ, {'OPENAI_API_KEY': 'test-key'})
    def test_analyze_directory(self):
        """Test directory analysis"""
        # Create temporary directory with exploit files
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a couple of test files
            exploit1 = os.path.join(temp_dir, "exploit1.py")
            exploit2 = os.path.join(temp_dir, "exploit2.rb")
            non_exploit = os.path.join(temp_dir, "readme.txt")
            
            with open(exploit1, 'w') as f:
                f.write("# Python exploit\nprint('test')")
            
            with open(exploit2, 'w') as f:
                f.write("# Ruby exploit\nputs 'test'")
            
            with open(non_exploit, 'w') as f:
                f.write("This is just a readme file")
            
            # Mock the analysis
            with patch('exploit_analyzer.ChatOpenAI'), \
                 patch.object(ExploitAnalyzer, '_setup_analysis_chain'), \
                 patch.object(ExploitAnalyzer, 'analyze_file') as mock_analyze:
                
                mock_analysis = ExploitAnalysis(
                    file_path="test",
                    vulnerability_type="Test",
                    cve_id=None,
                    severity="Medium",
                    target_software="Test",
                    attack_vector="Test",
                    impact_description="Test",
                    technical_details={},
                    payloads_identified=[],
                    indicators_of_compromise=[],
                    mitigation_recommendations=[],
                    confidence_score=0.8,
                    analysis_timestamp="2023-01-01T00:00:00"
                )
                mock_analyze.return_value = mock_analysis
                
                analyzer = ExploitAnalyzer()
                results = analyzer.analyze_directory(temp_dir)
                
                # Should analyze Python and Ruby files, but not txt
                self.assertEqual(len(results), 2)
                self.assertEqual(mock_analyze.call_count, 2)


def demo_analysis():
    """Demonstrate the exploit analyzer with the existing CVE-2023-32235 exploit"""
    
    print("\n" + "="*80)
    print("EXPLOIT ANALYZER DEMONSTRATION")
    print("="*80)
    
    # Check if we have the exploit file
    exploit_file = "exploit_CVE-2023-32235.py"
    if not os.path.exists(exploit_file):
        print(f"Error: Exploit file not found: {exploit_file}")
        print("Please ensure the CVE-2023-32235 exploit file is in the current directory.")
        return
    
    # Check for OpenAI API key
    if not os.getenv("OPENAI_API_KEY"):
        print("Warning: OPENAI_API_KEY not set. This is just a demonstration.")
        print("To run actual analysis, set your OpenAI API key:")
        print("export OPENAI_API_KEY='your-api-key-here'")
        print("\nShowing what the analysis commands would look like:")
        print(f"# Basic analysis")
        print(f"python3 3-analyze-exploit.py {exploit_file}")
        print(f"\n# JSON output to stdout")
        print(f"python3 3-analyze-exploit.py {exploit_file} --json")
        print(f"\n# JSON output to file")
        print(f"python3 3-analyze-exploit.py {exploit_file} --json > results.json")
        print(f"\n# Save detailed results")
        print(f"python3 3-analyze-exploit.py {exploit_file} -o analysis_results.json")
        print(f"\n# Analyze directory")
        print(f"python3 3-analyze-exploit.py . --extensions .py .rb .c")
        print(f"\n# Directory analysis with JSON output")
        print(f"python3 3-analyze-exploit.py . --json --extensions .py")
        return
    
    try:
        print(f"Analyzing exploit: {exploit_file}")
        print("This may take a moment as we analyze the code with LangChain...")
        
        # Import and run the analyzer
        analyzer = ExploitAnalyzer()
        analysis = analyzer.analyze_file(exploit_file)
        
        # Print results
        print("\n" + "="*80)
        print("ANALYSIS RESULTS")
        print("="*80)
        print(f"File: {analysis.file_path}")
        print(f"Vulnerability: {analysis.vulnerability_type}")
        print(f"CVE: {analysis.cve_id}")
        print(f"Severity: {analysis.severity}")
        print(f"Target: {analysis.target_software}")
        print(f"Confidence: {analysis.confidence_score:.2f}")
        
        if analysis.payloads_identified:
            print(f"\nPayloads Found:")
            for payload in analysis.payloads_identified[:3]:  # Show first 3
                print(f"  - {payload}")
        
        if analysis.mitigation_recommendations:
            print(f"\nTop Mitigations:")
            for rec in analysis.mitigation_recommendations[:3]:  # Show first 3
                print(f"  - {rec}")
        
        print(f"\nFull analysis saved to: analysis_demo.json")
        
        # Save results
        from exploit_analyzer import save_analysis_results
        save_analysis_results([analysis], "analysis_demo.json")
        
    except Exception as e:
        print(f"Error during analysis: {e}")


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "demo":
        demo_analysis()
    else:
        print("Running unit tests...")
        unittest.main()
